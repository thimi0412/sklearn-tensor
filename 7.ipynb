{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# アンサンブル学習とランダムフォレスト"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一群の予測機(分析器や回帰器)の予測を一つにまとめる、もっと優れている予測器の答えの方が良い予測を得られることが多い、この予測器のグループを`アンサンブル`と呼ぶ。この手法を`アンサンブル学習`、アンサンブル学習アルゴリズムを`アンサンブルメソッド`と呼ぶ\n",
    "\n",
    "例として、訓練セットから無作為に作った様々サブセットを使って一連の決定木分類器を訓練し、予測するときには全ての決定木の予測を集め、多数決で予測クラスを決める。このような決定木のアンサンブルを`ランダムフォレスト`と呼ぶ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=42, solver='liblinear',\n",
       "          tol=0.0001, verbose=0, warm_start=False)), ('rf', Rando...f',\n",
       "  max_iter=-1, probability=False, random_state=42, shrinking=True,\n",
       "  tol=0.001, verbose=False))],\n",
       "         flatten_transform=None, n_jobs=None, voting='hard', weights=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_clf = LogisticRegression(solver=\"liblinear\", random_state=42)\n",
    "rnd_clf = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "svm_clf = SVC(gamma=\"auto\", random_state=42)\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
    "    voting='hard')\n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.864\n",
      "RandomForestClassifier 0.872\n",
      "SVC 0.888\n",
      "VotingClassifier 0.896\n"
     ]
    }
   ],
   "source": [
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "投票分類器は個別の分類器のどれよりもわずかながら高い性能を示している。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "全ての分類器がクラスに属する確率を推計できるメソッド(`predict_proba()`)がある場合、個別の分類器の確率を平均し、もっとも確率が高いクラスを返すことができるようにする。これを`ソフト投票`と呼ぶ。この方法だと、自身の高い投票の重みがますため、ハード投票よりも高い性能を示すことが多い。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=42, solver='liblinear',\n",
       "          tol=0.0001, verbose=0, warm_start=False)), ('rf', Rando...bf',\n",
       "  max_iter=-1, probability=True, random_state=42, shrinking=True,\n",
       "  tol=0.001, verbose=False))],\n",
       "         flatten_transform=None, n_jobs=None, voting='soft', weights=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# votingをsoftにする\n",
    "# SVCのprobabilityハイパーパラメータをTrueにする\n",
    "log_clf = LogisticRegression(solver=\"liblinear\", random_state=42)\n",
    "rnd_clf = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "svm_clf = SVC(gamma=\"auto\", probability=True, random_state=42)\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
    "    voting='soft')\n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.864\n",
      "RandomForestClassifier 0.872\n",
      "SVC 0.888\n",
      "VotingClassifier 0.912\n"
     ]
    }
   ],
   "source": [
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ハード投票よりも高い性能が出た"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## バギングとペースティング\n",
    "訓練セットから無作為に別々のサブセットをサンプリングして訓練するというアプローチもある。サンプリングが重複でありで行われるときは`バギング`、重複なしで行われるときは`ペースティング`と呼ぶ。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "全ての分類器を予測したら、アンサンブルは単純に全ての予測器の予測を集計して新しいインスタンスに対する予測をする。  \n",
    "集計関数は、一般的に分類では`モード`(ハード投票と同じく予測の最頻値をとる)、回帰は平均。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scikit-learnにおけるバギングとペースティング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
